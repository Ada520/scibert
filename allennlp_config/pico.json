// Configuration adapted from https://gist.github.com/joelgrus/7cdb8fb2d81483a8d9ca121d9c617514
// for BERT embeddings CRF tagger
// Changed settings to use those specified in EBM-NLP paper
{
  "random_seed": std.extVar("SEED"),
  "pytorch_seed": std.extVar("PYTORCH_SEED"),
  "numpy_seed": std.extVar("NUMPY_SEED"),
  "dataset_reader": {
    "type": "ebmnlp",
    "token_indexers": {
      "bert": {
        "type": "bert-pretrained",
        "pretrained_model": std.extVar("BERT_VOCAB"),
        "do_lowercase": std.extVar("is_lowercase"),
        "use_starting_offsets": true
      }
    }
  },
  "train_data_path": std.extVar("TRAIN_PATH"),
  "validation_data_path": std.extVar("DEV_PATH"),
  "test_data_path": std.extVar("TEST_PATH"),
  "evaluate_on_test": true,
  "model": {
    "type": "pico_crf_tagger",
    "dropout": 0.5,
    "text_field_embedder": {
        "allow_unmatched_keys": true,
        "embedder_to_indexer_map": {
            "bert": ["bert", "bert-offsets"]
        },
        "token_embedders": {
            "bert": {
                "type": "bert-pretrained",
                "pretrained_model": std.extVar("BERT_WEIGHTS")
            }
        }
    },
    "encoder": {
        "type": "lstm",
        "input_size": 768, // bert size
        "hidden_size": 200,
        "num_layers": 1,
        "dropout": 0,
        "bidirectional": true
    }
  },
  "iterator": {
    "type": "basic",
    "batch_size": 20
  },
  "trainer": {
    "optimizer": {
        "type": "adam",
        "lr": 0.001
    },
    "validation_metric": "+avg_f1",
    "num_serialized_models_to_keep": 3,
    "num_epochs": 15,
    "grad_norm": 5.0,
    "patience": 3,
    "cuda_device": 0
  }
}
